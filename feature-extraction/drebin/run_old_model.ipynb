{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import math\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of dataset1, with 'malware' in class column\n",
    "# renamed to 0 and 'benign' renamed to 1\n",
    "df = pd.read_csv('dataset2.csv')\n",
    "\n",
    "# find pearson correlation for all features\n",
    "mean_class = df['class'].mean()\n",
    "column_names = df.columns\n",
    "\n",
    "sum1 = 0\n",
    "sum2 = 0\n",
    "sum3 = 0\n",
    "results = {}\n",
    "for column_name in column_names:\n",
    "    if(column_name != 'class'):\n",
    "        mean_of_attribute = df[column_name].mean(numeric_only=True)\n",
    "        sum1 = 0\n",
    "        sum2 = 0\n",
    "        sum3 = 0\n",
    "        for index, row in df.iterrows():\n",
    "            x_i = df.loc[index, column_name]\n",
    "            x_bar = mean_of_attribute\n",
    "            y_i = row['class']\n",
    "            y_bar = mean_class\n",
    "            sum1 += (x_i - x_bar)*(y_i-y_bar)\n",
    "            sum2 +=(x_i - x_bar)*(x_i - x_bar)\n",
    "            sum3+=(y_i-y_bar)*(y_i-y_bar)\n",
    "        if sum2*sum3 == 0:\n",
    "            correlation = 0\n",
    "        else:\n",
    "            correlation = sum1/(math.sqrt(sum2*sum3))\n",
    "        results[column_name] = correlation\n",
    "#sorted in increasing order\n",
    "results = dict(sorted(results.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set with all features saved, feature vector size: 214 (after \n",
    "# taking out class feature from original 216)\n",
    "result_set_1=df.drop('class', axis=1)\n",
    "result_set_1=result_set_1.drop('TelephonyManager.getSimCountryIso', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.model_selection import ShuffleSplit\\nX = result_set_1\\ny = df['class']\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0, stratify = y)\\nscaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)\\nsvc=SVC(C=10, gamma =0.1)\\nsvc.fit(X_train,y_train)\\ny_pred=svc.predict(X_test)\\ny_train_pred = svc.predict(X_train)\\nparam_grid = {'C': [0.1, 1, 10, 100, 1000], \\n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\\n              'degree': [2,3,4,5],\\n              'kernel': ['rbf', 'linear','sigmoid', 'poly']} \\n\\ngrid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 2, cv = 2, n_jobs=-1)\\n  \\n# fitting the model for grid search\\ngrid.fit(X_train, y_train)\\nprint(grid.best_params_)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMENTED OUT, only needs to run once + very computationally expensive\n",
    "# runs GridSearch with cross-fold val. = 2, full feature set, split rate 0.2\n",
    "\"\"\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "X = result_set_1\n",
    "y = df['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0, stratify = y)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "svc=SVC(C=10, gamma =0.1)\n",
    "svc.fit(X_train,y_train)\n",
    "y_pred=svc.predict(X_test)\n",
    "y_train_pred = svc.predict(X_train)\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'degree': [2,3,4,5],\n",
    "              'kernel': ['rbf', 'linear','sigmoid', 'poly']} \n",
    "\n",
    "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 2, cv = 2, n_jobs=-1)\n",
    "  \n",
    "# fitting the model for grid search\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Set 1\n",
      " Evaluating classifier on test data ...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1896\n",
      "           1       0.99      0.98      0.98      1112\n",
      "\n",
      "    accuracy                           0.99      3008\n",
      "   macro avg       0.99      0.98      0.99      3008\n",
      "weighted avg       0.99      0.99      0.99      3008\n",
      "\n",
      "Validation Accuracy: 0.9870345744680851\n",
      "Training Accuracy: 0.9988360492184902\n"
     ]
    }
   ],
   "source": [
    "# varying feature sets with a constant split rate of 0.2\n",
    "\n",
    "X = result_set_1\n",
    "# create the classifier\n",
    "y = df['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0, stratify = y)\n",
    "svc=SVC(C=10, gamma =0.1)\n",
    "svc.fit(X_train,y_train)\n",
    "y_pred=svc.predict(X_test)\n",
    "y_train_pred = svc.predict(X_train)\n",
    "\n",
    "# log results\n",
    "print(\"Result Set\",i+1)\n",
    "print(\" Evaluating classifier on test data ...\")\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(\"Validation Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Training Accuracy:\",metrics.accuracy_score(y_train, y_train_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.89      0.80       500\n",
      "           1       0.99      0.97      0.98      4821\n",
      "\n",
      "    accuracy                           0.96      5321\n",
      "   macro avg       0.86      0.93      0.89      5321\n",
      "weighted avg       0.96      0.96      0.96      5321\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data = pd.read_csv('output.csv')\n",
    "new_data=new_data.drop('TelephonyManager.getSimCountryIso', axis=1)\n",
    "X = new_data.drop('class', axis=1)\n",
    "# create the classifier\n",
    "y = new_data['class']\n",
    "y_pred=svc.predict(X)\n",
    "\n",
    "print(metrics.classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv('output.csv')\n",
    "new_data=new_data.drop('TelephonyManager.getSimCountryIso', axis=1)\n",
    "X = new_data.drop('class', axis=1)\n",
    "# create the classifier\n",
    "y = new_data['class']\n",
    "y_pred=svc.predict(X)\n",
    "\n",
    "print(metrics.classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39m# create the classifier\u001b[39;00m\n\u001b[1;32m      5\u001b[0m y \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m X_test \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49mtransform(X)\n\u001b[1;32m      7\u001b[0m y_pred\u001b[39m=\u001b[39msvc\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(metrics\u001b[39m.\u001b[39mclassification_report(y, y_pred))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:989\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\u001b[39mself\u001b[39m, X, copy\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    975\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Perform standardization by centering and scaling.\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \n\u001b[1;32m    977\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[39m        Transformed array.\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m     check_is_fitted(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    991\u001b[0m     copy \u001b[39m=\u001b[39m copy \u001b[39mif\u001b[39;00m copy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy\n\u001b[1;32m    992\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[1;32m    993\u001b[0m         X,\n\u001b[1;32m    994\u001b[0m         reset\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    998\u001b[0m         force_all_finite\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    999\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/utils/validation.py:1390\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1385\u001b[0m     fitted \u001b[39m=\u001b[39m [\n\u001b[1;32m   1386\u001b[0m         v \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m \u001b[39mvars\u001b[39m(estimator) \u001b[39mif\u001b[39;00m v\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m v\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1387\u001b[0m     ]\n\u001b[1;32m   1389\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fitted:\n\u001b[0;32m-> 1390\u001b[0m     \u001b[39mraise\u001b[39;00m NotFittedError(msg \u001b[39m%\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(estimator)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m})\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
