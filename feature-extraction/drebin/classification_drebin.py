import pandas as pd
import numpy as np
import time
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer as CountV
from sklearn.svm import LinearSVC
import os
from matplotlib import pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import recall_score, accuracy_score
from sklearn.utils import shuffle
import CommonModules as CM
import sys
import argparse
# imports
import pandas as pd
import math
import copy
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn import metrics
import random
"""
This script performs Drebin's classification using SVM LinearSVC classifier.
Inputs are described in parseargs function.
Recall and Accuracy are calculated 10 times. Each experiment is performed with 66% of training set and 33% of test set, and scores are averaged.
Roc curve is plotted using the last trained classifier from the 10 experiments.
The Outputs are the results text file, and roc curve pdf graph that are located in the drebin directory.
"""


def parseargs():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-md",
        "--maldir",
        help="The path to the .data malware files directory",
        type=str,
        required=True
    )
    parser.add_argument(
        "-gd",
        "--goodir",
        help="The path to the .data goodware files directory",
        type=str,
        required=True
    )
    parser.add_argument(
        "-fs",
        "--filescores",
        help="The name of the file where the results will be written",
        type=str,
        required=True
    )
    parser.add_argument(
        "-roc",
        "--roc",
        help="The name of the file where the roc curve will be saved",
        type=str,
        required=True
    )
    parser.add_argument(
        "-sd",
        "--seed",
        help=
        "(Optional) The random seed used for the experiments (useful for 100% identical results replication)",
        type=int,
        required=False,
        default=np.random.randint(0, 2**32 - 1)
    )
    args = parser.parse_args()
    return args


def Classification(MalwarePath, GoodwarePath, file_scores, ROC_GraphName):
    Malware = MalwarePath
    Goodware = GoodwarePath

    # the list of absolute paths of the ".data" malware files
    AllMalSamples = CM.ListFiles(MalwarePath, Extension)

    # the list of absolute paths of the ".data" goodware files
    AllGoodSamples = CM.ListFiles(GoodwarePath, Extension)
    AllSampleNames = AllMalSamples + AllGoodSamples  #combine the two lists

    FeatureVectorizer = CountV(
        input='filename',
        tokenizer=lambda x: x.split('\n'),
        token_pattern=None,
        binary=True
    )
    Mal_labels = np.ones(len(AllMalSamples))  #label malware as 1
    Good_labels = np.empty(len(AllGoodSamples))
    Good_labels.fill(-1)  # label goodware as -1
    # concatenate the two lists of labels
    y = np.concatenate((Mal_labels, Good_labels), axis=0)
    scoresRec, scoresAcc = [], []  # empty lists to store recall and accuracy
    # The experiment is run 10 times


    # create dataframe of dataset1, with 'malware' in class column
    # renamed to 0 and 'benign' renamed to 1
    df = pd.read_csv('dataset2.csv')

    # find pearson correlation for all features
    mean_class = df['class'].mean()
    column_names = df.columns

    sum1 = 0
    sum2 = 0
    sum3 = 0
    results = {}
    for column_name in column_names:
        if(column_name != 'class'):
            mean_of_attribute = df[column_name].mean(numeric_only=True)
            sum1 = 0
            sum2 = 0
            sum3 = 0
            for index, row in df.iterrows():
                x_i = df.loc[index, column_name]
                x_bar = mean_of_attribute
                y_i = row['class']
                y_bar = mean_class
                sum1 += (x_i - x_bar)*(y_i-y_bar)
                sum2 +=(x_i - x_bar)*(x_i - x_bar)
                sum3+=(y_i-y_bar)*(y_i-y_bar)
            if sum2*sum3 == 0:
                correlation = 0
            else:
                correlation = sum1/(math.sqrt(sum2*sum3))
            results[column_name] = correlation
    #sorted in increasing order
    results = dict(sorted(results.items(), key=lambda item: item[1]))

    # set with all features saved, feature vector size: 214 (after 
    # taking out class feature from original 216)
    result_set_1=df.drop('class', axis=1)
    result_set_1=result_set_1.drop('TelephonyManager.getSimCountryIso', axis=1)
    results_copy = copy.deepcopy(results)
    results_iter = copy.deepcopy(results)

    # correlation threshold >= 0.1, feature vector size: 39
    result_set_2 = result_set_1.copy()
    for feature in results_iter:
        if results_iter[feature] < 0.05:
            del results_copy[feature]
            result_set_2.drop(feature, axis=1, inplace=True)

    # varying feature sets with a constant split rate of 0.2
    result_sets = [result_set_1] 
    for i, X in enumerate(result_sets):
        # create the classifier
        y = df['class']
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0, stratify = y)
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)
        svc=SVC(C=10, gamma =0.1)
        svc.fit(X_train,y_train)
        y_pred=svc.predict(X_test)
        y_train_pred = svc.predict(X_train)

        new_data = pd.read_csv('output.csv')
        new_data=new_data.drop('TelephonyManager.getSimCountryIso', axis=1)
        # Extract the features (X_new) and the labels (y_new) from the new data
        X_new = new_data.drop('class', axis=1)
        y_new = new_data['class']

        # Scale the features using the same scaler used for training data
        X_new_scaled = scaler.transform(X_new)

        # Predict on the new data
        y_pred_new = svc.predict(X_new_scaled)

        scoreRec = recall_score(y_new, y_pred_new)  #calculate the recall score
        #calculate the accuracy score
        scoreAcc = accuracy_score(y_new, y_pred_new)
        #store the calculated recall in the list of recall scores
        scoresRec.append(scoreRec)
        #store the calculated accuarcy in the list of accuracy scores
        scoresAcc.append(scoreAcc)

        print("Accuracy", accuracy_score(y_new, y_pred_new))
        print("Recall", recall_score(y_new, y_pred_new))


Extension = ".data"  #features files that are generated by GetApkData.py script ends with ".data"

if __name__ == '__main__':
    Args = parseargs()  #retrieve the parameters
    MalwarePath = Args.maldir
    GoodwarePath = Args.goodir
    file_scores = Args.filescores
    ROC_GraphName = Args.roc
    SEED = Args.seed
    np.random.seed(SEED)
    Classification(MalwarePath, GoodwarePath, file_scores, ROC_GraphName)