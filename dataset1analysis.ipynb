{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of dataset1, with 'malware' in classification column\n",
    "# renamed to 0 and 'benign' renamed to 1\n",
    "df = pd.read_csv('dataset1.csv')\n",
    "\n",
    "# drop hash column, axis=1 specifies dropping a column not a row\n",
    "df = df.drop('hash', axis=1)\n",
    "\n",
    "# find pearson correlation for all features\n",
    "mean_classification = df['classification'].mean()\n",
    "column_names = df.columns\n",
    "\n",
    "sum1 = 0\n",
    "sum2 = 0\n",
    "sum3 = 0\n",
    "results = {}\n",
    "for column_name in column_names:\n",
    "    if(column_name != 'classification'):\n",
    "        mean_of_attribute = df[column_name].mean(numeric_only=True)\n",
    "        sum1 = 0\n",
    "        sum2 = 0\n",
    "        sum3 = 0\n",
    "        for index, row in df.iterrows():\n",
    "            x_i = df.loc[index, column_name]\n",
    "            x_bar = mean_of_attribute\n",
    "            y_i = row.classification\n",
    "            y_bar = mean_classification\n",
    "            sum1 += (x_i - x_bar)*(y_i-y_bar)\n",
    "            sum2 +=(x_i - x_bar)*(x_i - x_bar)\n",
    "            sum3+=(y_i-y_bar)*(y_i-y_bar)\n",
    "        if sum2*sum3 == 0:\n",
    "            correlation = 0\n",
    "        else:\n",
    "            correlation = sum1/(math.sqrt(sum2*sum3))\n",
    "        results[column_name] = correlation\n",
    "#sorted in increasing order\n",
    "results = dict(sorted(results.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set with all features saved, feature vector size: 33 (after \n",
    "# taking out hash and classification features from original 35)\n",
    "result_set_1=df.drop('classification', axis=1)\n",
    "# remove the 6 lowest-scoring features, feature vector size: 27\n",
    "result_set_2 = result_set_1.copy()\n",
    "result_set_2 = result_set_2.drop('vm_truncate_count', axis=1)\n",
    "result_set_2 = result_set_2.drop('shared_vm', axis=1)\n",
    "result_set_2 = result_set_2.drop('exec_vm', axis=1)\n",
    "result_set_2 = result_set_2.drop('nvcsw', axis=1)\n",
    "result_set_2 = result_set_2.drop('maj_flt', axis=1)\n",
    "result_set_2 = result_set_2.drop('utime', axis=1)\n",
    "\n",
    "# remove the next 3 lowest-scoring features, feature vector size: 24\n",
    "result_set_3 = result_set_2.copy()\n",
    "result_set_3 = result_set_3.drop('static_prio', axis=1)\n",
    "result_set_3 = result_set_3.drop('map_count', axis=1)\n",
    "result_set_3 = result_set_3.drop('end_data', axis=1)\n",
    "\n",
    "# remove the next 3 lowest-scoring features, feature vector size: 21\n",
    "result_set_4 = result_set_3.copy()\n",
    "result_set_4 = result_set_4.drop('nivcsw', axis=1)\n",
    "result_set_4 = result_set_4.drop('fs_excl_counter', axis=1)\n",
    "result_set_4 = result_set_4.drop('reserved_vm', axis=1)\n",
    "\n",
    "# remove all remaining features with negative correlation, feature vector size: 14\n",
    "result_set_5 = result_set_4.copy()\n",
    "result_set_5 = result_set_5.drop('mm_users', axis=1)\n",
    "result_set_5 = result_set_5.drop('state', axis=1)\n",
    "result_set_5 = result_set_5.drop('total_vm', axis=1)\n",
    "result_set_5 = result_set_5.drop('free_area_cache', axis=1)\n",
    "result_set_5 = result_set_5.drop('stime', axis=1)\n",
    "result_set_5 = result_set_5.drop('gtime', axis=1)\n",
    "result_set_5 = result_set_5.drop('millisecond', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see distribution\n",
    "print(\"Show the distribution of malware (1) and goodware (0):\")\n",
    "print(df['classification'].value_counts()/((len(df))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMENTED OUT, only needs to run once + very computationally expensive\n",
    "# runs GridSearch with cross-fold val. = 2, full feature set, split rate 0.2\n",
    "\"\"\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "X = result_set_1\n",
    "y = df['classification']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0, stratify = y)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "svc=SVC(C=10, gamma =0.1)\n",
    "svc.fit(X_train,y_train)\n",
    "y_pred=svc.predict(X_test)\n",
    "y_train_pred = svc.predict(X_train)\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'degree': [2,3,4,5],\n",
    "              'kernel': ['rbf', 'linear','sigmoid', 'poly']} \n",
    "\n",
    "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 2, cv = 2, n_jobs=-1)\n",
    "  \n",
    "# fitting the model for grid search\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# varying feature sets with a constant split rate of 0.2\n",
    "result_sets = [result_set_1, result_set_2, result_set_3,result_set_4,result_set_5] \n",
    "for i, X in enumerate(result_sets):\n",
    "    # create the classifier\n",
    "    y = df['classification']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0, stratify = y)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    svc=SVC(C=10, gamma =0.1)\n",
    "    svc.fit(X_train,y_train)\n",
    "    y_pred=svc.predict(X_test)\n",
    "    y_train_pred = svc.predict(X_train)\n",
    "\n",
    "    # log results\n",
    "    print(\"Result Set\",i+1)\n",
    "    print(\" Evaluating classifier on test data ...\")\n",
    "    print(metrics.classification_report(y_test, y_pred))\n",
    "    print(\"Validation Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "    print(\"Training Accuracy:\",metrics.accuracy_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# varying split rate with a constant feature set of vector size 21\n",
    "split_rate = [0.2, 0.25, 0.3] \n",
    "for i in split_rate:\n",
    "    # create the classifier\n",
    "    X = result_set_4\n",
    "    y = df['classification']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = i, random_state = 0, stratify = y)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    svc=SVC(C=10, gamma =0.1)\n",
    "    svc.fit(X_train,y_train)\n",
    "    y_pred=svc.predict(X_test)\n",
    "    y_train_pred = svc.predict(X_train)\n",
    "\n",
    "    # log results\n",
    "    print(\"Result Set\", i+1)\n",
    "    print(\" Evaluating classifier on test data ...\")\n",
    "    print(metrics.classification_report(y_test, y_pred))\n",
    "    print(\"Validation Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "    print(\"Training Accuracy:\",metrics.accuracy_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random set 1\n",
      "Average weighted precision: 0.9998700409729089\n",
      "Average weighted F1-score: 0.9998699999932249\n",
      "Average weighted recall: 0.9998699999999999\n",
      "Average accuracy: 0.9998699999999999\n",
      "Average T.acc%: 0.9998699999999999\n",
      "Average V.acc%: 0.9998875\n",
      "Random set 2\n",
      "Average weighted precision: 0.9997501867031877\n",
      "Average weighted F1-score: 0.9997499999257251\n",
      "Average weighted recall: 0.99975\n",
      "Average accuracy: 0.99975\n",
      "Average T.acc%: 0.99975\n",
      "Average V.acc%: 0.9998575000000001\n",
      "Random set 3\n",
      "Average weighted precision: 0.9995201576120077\n",
      "Average weighted F1-score: 0.99951999990295\n",
      "Average weighted recall: 0.9995200000000001\n",
      "Average accuracy: 0.9995200000000001\n",
      "Average T.acc%: 0.9995200000000001\n",
      "Average V.acc%: 0.9995574999999999\n",
      "Random set 4\n",
      "Average weighted precision: 0.9910923034410535\n",
      "Average weighted F1-score: 0.9909179765671631\n",
      "Average weighted recall: 0.99092\n",
      "Average accuracy: 0.99092\n",
      "Average T.acc%: 0.99092\n",
      "Average V.acc%: 0.9912050000000001\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "# creating list of all 33 features corresponding to an index\n",
    "feature_vector_ordered = []\n",
    "for feature in results:\n",
    "    feature_vector_ordered.append(feature)\n",
    "# skipping random_feature_set_1 because the other result_set_1 corresponds\n",
    "# to the original set of feature vectors\n",
    "# the randomized 5 sets that will hold 27 features\n",
    "random_set_2_1 = result_set_1.copy()\n",
    "random_set_2_2 = result_set_1.copy()\n",
    "random_set_2_3 = result_set_1.copy()\n",
    "random_set_2_4 = result_set_1.copy()\n",
    "random_set_2_5 = result_set_1.copy()\n",
    "random_feature_set_2 = [random_set_2_1, random_set_2_2, random_set_2_3, random_set_2_4, random_set_2_5]\n",
    "# the randomized 5 sets that will hold 24 features\n",
    "random_set_3_1 = result_set_1.copy()\n",
    "random_set_3_2 = result_set_1.copy()\n",
    "random_set_3_3 = result_set_1.copy()\n",
    "random_set_3_4 = result_set_1.copy()\n",
    "random_set_3_5 = result_set_1.copy()\n",
    "random_feature_set_3 = [random_set_3_1, random_set_3_2, random_set_3_3, random_set_3_4, random_set_3_5]\n",
    "# the randomized 5 sets that will hold 21 features\n",
    "random_set_4_1 = result_set_1.copy()\n",
    "random_set_4_2 = result_set_1.copy()\n",
    "random_set_4_3 = result_set_1.copy()\n",
    "random_set_4_4 = result_set_1.copy()\n",
    "random_set_4_5 = result_set_1.copy(4)\n",
    "random_feature_set_4 = [random_set_4_1, random_set_4_2, random_set_4_3, random_set_4_4, random_set_4_5]\n",
    "# the randomized 5 sets that will hold 14 features\n",
    "random_set_5_1 = result_set_1.copy()\n",
    "random_set_5_2 = result_set_1.copy()\n",
    "random_set_5_3 = result_set_1.copy()\n",
    "random_set_5_4 = result_set_1.copy()\n",
    "random_set_5_5 = result_set_1.copy()\n",
    "random_feature_set_5 = [random_set_5_1, random_set_5_2, random_set_5_3, random_set_5_4, random_set_5_5]\n",
    "# randomly select the appropriate number of features for each subset, so that each \n",
    "# random_feature_set holds 5 subsets of randomly selected features\n",
    "random_set = [random_feature_set_2, random_feature_set_3, random_feature_set_4, random_feature_set_5]\n",
    "num_features_to_drop = [6, 9, 12, 19]\n",
    "i = 0\n",
    "for rs in random_set:\n",
    "    for random_subset in rs:\n",
    "        dropped_features = []\n",
    "        while len(dropped_features) < num_features_to_drop[i]:\n",
    "            rand_num = random.randint(1, 33) - 1\n",
    "            if rand_num not in dropped_features:\n",
    "                random_subset.drop(feature_vector_ordered[rand_num], axis=1, inplace=True)\n",
    "                dropped_features.append(rand_num)\n",
    "    i+=1\n",
    "\n",
    "# for each feature vector size (27, 24, 21, 14) use the randomly chosen feature subsets\n",
    "# (five for each vector size) and average the results\n",
    "count = 1\n",
    "for rs in random_set:\n",
    "    avg_weighted_precision = 0\n",
    "    avg_weighted_f1 = 0\n",
    "    avg_weighted_recall = 0\n",
    "    avg_accuracy = 0\n",
    "    avg_tacc = 0\n",
    "    avg_vacc = 0\n",
    "    for i, X in enumerate(rs):\n",
    "        # create the classifier\n",
    "        y = df['classification']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0, stratify = y)\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        svc=SVC(C=10, gamma =0.1)\n",
    "        svc.fit(X_train,y_train)\n",
    "        y_pred=svc.predict(X_test)\n",
    "        y_train_pred = svc.predict(X_train)\n",
    "        report = metrics.classification_report(y_test, y_pred,output_dict=True)\n",
    "        avg_weighted_precision += report['weighted avg']['precision'] \n",
    "        avg_weighted_f1 += report['weighted avg']['f1-score'] \n",
    "        avg_accuracy += report['accuracy']\n",
    "        avg_weighted_recall += report['weighted avg']['recall'] \n",
    "        avg_tacc += metrics.accuracy_score(y_test, y_pred)\n",
    "        avg_vacc += metrics.accuracy_score(y_train, y_train_pred)\n",
    "    avg_weighted_precision /= 5\n",
    "    avg_weighted_f1 /= 5\n",
    "    avg_weighted_recall /= 5\n",
    "    avg_accuracy /= 5\n",
    "    avg_tacc /= 5\n",
    "    avg_vacc /= 5\n",
    "    print(\"Random set\", count)\n",
    "    print(\"Average weighted precision:\",avg_weighted_precision)\n",
    "    print(\"Average weighted F1-score:\",avg_weighted_f1)\n",
    "    print(\"Average weighted recall:\",avg_weighted_recall)\n",
    "    print(\"Average accuracy:\",avg_accuracy)\n",
    "    print(\"Average T.acc%:\",avg_tacc)\n",
    "    print(\"Average V.acc%:\",avg_vacc)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
